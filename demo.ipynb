{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sF6ziz96BQV",
        "colab_type": "text"
      },
      "source": [
        "# Colab Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE74AaXJPh1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/MikeCun/PersonReID.git\n",
        "!mv PersonReID/* ./\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWXEkzqtPSyp",
        "colab_type": "text"
      },
      "source": [
        "# Person ReID Demo\n",
        "\n",
        "A quick intro to using the pre-trained model of Mask RCNN and EANET to re-idefication the person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HtCug01xBPOC",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"./\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "# Import COCO config\n",
        "from mrcnn import coco\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFFljOcpPSyy",
        "colab_type": "text"
      },
      "source": [
        "# Configurations, Create Model and Load Trained Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ixxkAAKMBrOy",
        "colab": {}
      },
      "source": [
        "class InferenceConfig(coco.CocoConfig):\n",
        "    # Set batch size to 1 since we'll be running inference on\n",
        "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "\n",
        "# Create model object in inference mode.\n",
        "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
        "\n",
        "# Load weights trained on MS-COCO\n",
        "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
        "\n",
        "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
        "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
        "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
        "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
        "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
        "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "               'teddy bear', 'hair drier', 'toothbrush']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IprOs1akPSy1",
        "colab_type": "text"
      },
      "source": [
        "# Run Person Detection and Store the feature\n",
        "\n",
        "we use the Mask RCNN to detect the person in every frame.Then using the EANET to extract the feature of them and storing in 'feature_file.h5' by LSH."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eQ5BX7ERBwgO",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import h5py\n",
        "import torch\n",
        "import pickle\n",
        "import warnings\n",
        "import progressbar\n",
        "import numpy as np\n",
        "from easydict import EasyDict\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from package.eval.np_distance import compute_dist\n",
        "from package.optim.eanet_trainer import EANetTrainer\n",
        "from face_recognition.api import compare_faces as face_com\n",
        "from face_recognition.api import face_locations as face_loc\n",
        "from face_recognition.api import face_encodings as face_enc\n",
        "from face_recognition.api import load_image_file as face_load\n",
        "\n",
        "# Prepare load model because of encoding method of the EANET model\n",
        "pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
        "pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Prepare the EANET\n",
        "args = EasyDict()\n",
        "args.exp_dir = './' #EANET Model Path\n",
        "args.cfg_file = 'package/config/default.py'\n",
        "args.ow_file = 'paper_configs/PCB.txt'\n",
        "args.ow_str = \"cfg.only_infer = True\"\n",
        "\n",
        "eanet_trainer = EANetTrainer(args=args)\n",
        "\n",
        "# Set the workpath\n",
        "general_workpath = os.path.join(os.getcwd(), 'reid_result/') # General workpath\n",
        "workvideo = os.path.join(os.getcwd(), \"test_2.avi\")\n",
        "videoname = os.path.splitext(os.path.split(workvideo)[1])[0]\n",
        "\n",
        "# Move to videoname file\n",
        "newworkpath = os.path.join(general_workpath, videoname) + '/'\n",
        "if not os.path.exists(newworkpath):\n",
        "    os.makedirs(os.path.join(newworkpath))\n",
        "\n",
        "# Open the video and get the total frame\n",
        "capture = cv2.VideoCapture(workvideo)\n",
        "total_frame = int(capture.get(7)) + 1\n",
        "\n",
        "# Initial the some variable\n",
        "p = progressbar.ProgressBar() # Show the exacting feature progress\n",
        "p.start(total_frame)\n",
        "is_stopped = True\n",
        "# GPUs can speed the progress of exacting face feature\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "# Create the dictionary to store the feature and their path\n",
        "body_feature_dict = {'path':[], 'feat':[]}\n",
        "face_feature_dict = {'path':[], 'feat':[]}\n",
        "\n",
        "# Default frequence is 1 sec\n",
        "fps = int(capture.get(5))\n",
        "frame_freq = fps\n",
        "# frame_freq = 8\n",
        "\n",
        "def frame_to_time(frame_now, fps=fps):\n",
        "    secs = frame_now // fps\n",
        "    s, ms = divmod(frame_now, fps)\n",
        "    m, s = divmod(secs, 60)\n",
        "    h, m = divmod(m, 60)\n",
        "    time_stamp = (\"%02d-%02d-%02d-%03d\" % (h, m, s, ms))\n",
        "    return time_stamp\n",
        "\n",
        "while is_stopped:\n",
        "    \n",
        "    # Load the video frame and decide when the video is end by 'is_stopped'\n",
        "    is_stopped, frame = capture.read()\n",
        "    frame_now = int(capture.get(1))\n",
        "    if (frame_now % frame_freq == 0 and is_stopped):\n",
        "        \n",
        "        results = model.detect([frame], verbose=0) # Detection\n",
        "        r = results[0]\n",
        "        # Visualization the detection result of the frame\n",
        "        # visualize.display_instances(frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])\n",
        "        \n",
        "        for i in range(r['rois'].shape[0]):\n",
        "            # Get every pedestrian in the frame\n",
        "            if not np.any(r['rois'][i]):\n",
        "                continue\n",
        "            \n",
        "            # Get the pedestrian in the frame and possibility depending the detection result is greater than 0.95\n",
        "            if r['class_ids'][i] == 1 and r['scores'][i] >= 0.95:\n",
        "                y1, x1, y2, x2 = r['rois'][i] # Get the target area of pedestrian\n",
        "                \n",
        "                # Define the pic name of frame using time stamp in video and serial number in the frame detection result\n",
        "                pic_name = frame_to_time(frame_now) + '_' + str(i)\n",
        "                img_path = newworkpath + pic_name + '.jpg'\n",
        "                \n",
        "                # Store every pedestrian target area\n",
        "                if not os.path.exists(img_path):\n",
        "                    cv2.imwrite(img_path, frame[y1: y2, x1: x2])\n",
        "                \n",
        "                # Exact the feature of pedestrain body\n",
        "                body_feature = eanet_trainer.infer_one_im(im_path=img_path, squeeze=True)['feat']\n",
        "                body_feature_dict['path'].append(img_path.encode()) # h5py only supports ASCII in string\n",
        "                body_feature_dict['feat'].append(body_feature)      # encode() when is stored, decode() when for using\n",
        "                \n",
        "                # Using the face feature by Face Recognition API\n",
        "                face_np = face_load(img_path)\n",
        "                # If you don't have the GPUs, you can remove the 'model' params then it will use 'hog' instead of 'cnn'\n",
        "                face_location = face_loc(face_np, model=\"cnn\" if cuda else None)\n",
        "                if face_location: # Sometimes can't detect the face in the frame\n",
        "                    # Exact the face feature of pedestrian\n",
        "                    face_feature = face_enc(face_np, known_face_locations=face_location)[0]\n",
        "                    face_feature_dict['path'].append(img_path.encode()) # h5py only supports ASCII in string\n",
        "                    face_feature_dict['feat'].append(face_feature)      # encode() when is stored, decode() when for using\n",
        "                    \n",
        "    p.update(frame_now) # Update the progress\n",
        "    \n",
        "# Store the dictionary including the path and feature using h5\n",
        "with h5py.File(newworkpath + videoname + \"_body_feature_data.h5\", \"w\") as body_feature_file:\n",
        "    body_feature_file.create_dataset(videoname + '_path_data', data=body_feature_dict['path'])\n",
        "    body_feature_file.create_dataset(videoname + '_feat_data', data=body_feature_dict['feat'])\n",
        "\n",
        "with h5py.File(newworkpath + videoname + \"_face_feature_data.h5\", \"w\") as face_feature_file:\n",
        "    face_feature_file.create_dataset(videoname + '_path_data', data=face_feature_dict['path'])\n",
        "    face_feature_file.create_dataset(videoname + '_feat_data', data=face_feature_dict['feat'])\n",
        "    \n",
        "# Close the file and capture\n",
        "body_feature_file.close()\n",
        "face_feature_file.close()\n",
        "capture.release()\n",
        "p.finish()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwVPF0JOPSy5",
        "colab_type": "text"
      },
      "source": [
        "# Person Re-Identification\n",
        "we use lsh to search the person which has been extracted the feature and stored in file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "okvFomWLMyh4",
        "colab": {}
      },
      "source": [
        "# Set the ReID picture path\n",
        "reidpicpath = './reid_test_1.png'\n",
        "reid_image = cv2.imread(reidpicpath, cv2.IMREAD_COLOR)\n",
        "\n",
        "# Set the ReID workpath\n",
        "reidvideo_path = '/content/test_2.avi'\n",
        "videoname = os.path.splitext(os.path.split(reidvideo_path)[1])[0]\n",
        "if os.path.join(general_workpath, videoname):\n",
        "    reidworkpath = os.path.join(general_workpath, videoname) + '/'\n",
        "else: # If there isn't a feature file of the video\n",
        "    print(\"The %s video hasn't exacted the feature!\") % (videoname)\n",
        "    sys.exit(0)\n",
        "\n",
        "# Get the ReID picture name\n",
        "reidname = os.path.splitext(os.path.split(reidpicpath)[1])[0]\n",
        "\n",
        "# Detection\n",
        "reid_results = model.detect([reid_image], verbose=0)\n",
        "reid_r = reid_results[0]\n",
        "\n",
        "# Create the dictionary of ReID result including every detected pedestrian respectively\n",
        "reid_dict = {'reid_path':[], 'reid_rank':[]}\n",
        "reid_body_feature_dict = {'reid_path':[], 'reid_rank':[]}\n",
        "reid_face_feature_dict = {'reid_path':[], 'reid_rank':[]}\n",
        "\n",
        "\n",
        "# Visualization the detection result of ReID picture\n",
        "# visualize.display_instances(reid_image, reid_r['rois'], reid_r['masks'], reid_r['class_ids'], class_names, reid_r['scores'])\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Open the stored body and face feature respectively in the directory\n",
        "with h5py.File(reidworkpath + videoname + \"_body_feature_data.h5\", \"r\") as reid_body_feature_file:\n",
        "    reid_body_feature_dict['reid_path'] = reid_body_feature_file[videoname + '_path_data'].value\n",
        "    reid_body_feature_dict['reid_feat'] = reid_body_feature_file[videoname + '_feat_data'].value\n",
        "    \n",
        "with h5py.File(reidworkpath + videoname + \"_face_feature_data.h5\", \"r\") as reid_face_feature_file:\n",
        "    reid_face_feature_dict['reid_path'] = reid_face_feature_file[videoname + '_path_data'].value\n",
        "    reid_face_feature_dict['reid_feat'] = reid_face_feature_file[videoname + '_feat_data'].value\n",
        "    \n",
        "for i in range(reid_r['rois'].shape[0]):\n",
        "    # Progress every pedestrian in the ReID picture\n",
        "    if not np.any(reid_r['rois'][i]):\n",
        "        continue\n",
        "    \n",
        "    # Get the pedestrian in the ReID picture and possibility depending the detection result is greater than 0.95\n",
        "    if reid_r['class_ids'][i] == 1 and reid_r['scores'][i] >= 0.95:\n",
        "        y1, x1, y2, x2 = reid_r['rois'][i] # Get the target area of pedestrian\n",
        "        \n",
        "        # Define the pic name of ReID picture using serial number in the detection result\n",
        "        reid_pic_name = str(reidname) + '_' + str(i)\n",
        "        reid_img_path = reidworkpath + reid_pic_name + '.jpg'\n",
        "        reid_temp_list = [] # Using for adding the ReID result\n",
        "        \n",
        "        # Store every pedestrian target area\n",
        "        if not os.path.exists(reid_img_path):\n",
        "            cv2.imwrite(reid_img_path, reid_image[y1: y2, x1: x2])\n",
        "        \n",
        "        # Exact the feature of pedestrain body\n",
        "        reid_body_feature = eanet_trainer.infer_one_im(im_path=reid_img_path, squeeze=False)['feat']\n",
        "        # Using the cosine distence as indicator to distinguish the different pedestrian\n",
        "        # cosine_distance is a list of cosine distance among the ReID body feature and body feature which is stored in the directory\n",
        "        cosine_distance = compute_dist(reid_body_feature, reid_body_feature_dict['reid_feat'])[0]\n",
        "        # Distinguished distance as a threshold value can distinguish between two pedestrians if they are same one\n",
        "        distinguished_distance = 2 * np.mean(cosine_distance) - np.max(cosine_distance)\n",
        "        \n",
        "        for j in range(len(cosine_distance)):\n",
        "            if cosine_distance[j] <= distinguished_distance:\n",
        "                # Add the same pedestrians result by body disinguishing\n",
        "                reid_temp_list.append(reid_body_feature_dict['reid_path'][j])\n",
        "        \n",
        "        # Exact the feature of pedestrain face if he has\n",
        "        reid_face_np = face_load(reid_img_path)\n",
        "        reid_face_location = face_loc(reid_face_np, model=\"cnn\" if cuda else None)\n",
        "        if reid_face_location:\n",
        "            reid_face_feature = face_enc(reid_face_np, known_face_locations=reid_face_location)[0]\n",
        "            # reid_face_result is a list just including 'True' and 'False'\n",
        "            reid_face_result = face_com(reid_face_feature_dict['reid_feat'], reid_face_feature, tolerance=0.58)\n",
        "            \n",
        "            for k in range(len(reid_face_result)):\n",
        "                if reid_face_result[k]:\n",
        "                    # Add the same pedestrians result by face recognition\n",
        "                    reid_temp_list.append(reid_face_feature_dict['reid_path'][k])\n",
        "        \n",
        "        reid_dict['reid_path'].append(reid_img_path.encode()) # Add the picture path of ReID result\n",
        "        \n",
        "        if len(reid_temp_list) == 0: # If ReID fails, there is another picture to show the result\n",
        "            reid_dict['reid_rank'].append([(os.getcwd() + '/fail.jpg').encode()]) # h5py only supports ASCII in string\n",
        "        else:\n",
        "            reid_dict['reid_rank'].append(list(set(reid_temp_list))) # 'set' is used for removing repeat items\n",
        "            \n",
        "# Store the ReID list result for Visualization\n",
        "reid_result_file_path = reidworkpath + videoname + \"_\" + reidname + \"_reid_result.h5\"\n",
        "with h5py.File(reid_result_file_path, \"w\") as reid_result_file:\n",
        "    reid_result_file.create_dataset(reidname + '_reid_path', data=reid_dict['reid_path'])\n",
        "    reid_result_file.create_dataset(reidname + '_reid_rank', data=reid_dict['reid_rank'])\n",
        "    \n",
        "# Store the ReID result record for Visualization\n",
        "with h5py.File(general_workpath + \"reid_record.h5\", \"a\") as reid_record:\n",
        "    reid_record.create_dataset(videoname + \"-\" + reidpicpath, data=reid_result_file_path)\n",
        "    \n",
        "# Close the file which is opened\n",
        "reid_body_feature_file.close()\n",
        "reid_face_feature_file.close()\n",
        "reid_result_file.close()\n",
        "reid_record.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fCyQQ0i_ny8",
        "colab_type": "text"
      },
      "source": [
        "# Visualization ReID Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Pq_kaA_nB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the ReID picture and ReID result path for visualization\n",
        "visualize_reid_pic_path = './reid_test_1.png' # The Path can't be changed after ReID queries\n",
        "visualize_reid_video_path = os.path.join(os.getcwd(), \"test_2.avi\")\n",
        "reidvideoname = os.path.splitext(os.path.split(visualize_reid_video_path)[1])[0]\n",
        "vis_picname = os.path.splitext(os.path.split(visualize_reid_pic_path)[1])[0]\n",
        "import matplotlib.image as mpimg\n",
        "# Query the visualize_reid_pic_path in the stored reid_record to load the ReID result\n",
        "with h5py.File(general_workpath + \"reid_record.h5\", \"r\") as reid_record:\n",
        "    reid_result_file_path = reid_record[reidvideoname + \"-\" + visualize_reid_pic_path].value\n",
        "\n",
        "# Open the stored ReID list result in the directory\n",
        "reid_vis_dict = {'reid_path':[], 'reid_rank':[]}\n",
        "with h5py.File(reid_result_file_path, \"r\") as reid_vis_dict_file:\n",
        "    reid_vis_dict['reid_path'] = reid_vis_dict_file[vis_picname + '_reid_path'].value\n",
        "    reid_vis_dict['reid_rank'] = reid_vis_dict_file[vis_picname + '_reid_rank'].value\n",
        "    \n",
        "for i in range(len(reid_vis_dict['reid_path'])):\n",
        "    \n",
        "    plt.title(\"Query Picture: \" + vis_picname)\n",
        "    plt.imshow(mpimg.imread(reid_vis_dict['reid_path'][i].decode()))\n",
        "    plt.show()\n",
        "    \n",
        "    for j in range(len(reid_vis_dict['reid_rank'][i])):\n",
        "        reid_pic_path = reid_vis_dict['reid_rank'][i][j].decode()\n",
        "        reid_picname = os.path.splitext(os.path.split(reid_pic_path)[1])[0]   \n",
        "        plt.title(reid_picname)\n",
        "        plt.imshow(mpimg.imread(reid_pic_path))\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}